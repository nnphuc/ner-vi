{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner-vi nlp.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O4JH9iqj2tD-",
        "colab_type": "code",
        "outputId": "75c91e50-7363-4c78-be4b-8cb1cefe62c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RtCJTHHd4f0K",
        "colab_type": "code",
        "outputId": "0524bb2d-8f23-4b9f-a26b-a7838734c896",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n",
        "#!pip install image\n",
        "#!pip install scikit-image\n",
        "!pip install --no-cache-dir -I pillow\n",
        "#!pip install Pillow==4.0.0\n",
        "#!pip install pydensecrf\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 17.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pillow\n",
            "Successfully installed pillow-5.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vqC-0RRR3MFO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import sys\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SO29JdUo3faO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WCDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        :param wmaps: padded encoded word sequences\n",
        "        :param cmaps_f: padded encoded forward character sequences\n",
        "        :param cmaps_b: padded encoded backward character sequences\n",
        "        :param cmarkers_f: padded forward character markers\n",
        "        :param cmarkers_b: padded backward character markers\n",
        "        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n",
        "        :param wmap_lengths: word sequence lengths\n",
        "        :param cmap_lengths: character sequence lengths\n",
        "        \"\"\"\n",
        "        self.wmaps = wmaps\n",
        "        self.cmaps_f = cmaps_f\n",
        "        self.cmaps_b = cmaps_b\n",
        "        self.cmarkers_f = cmarkers_f\n",
        "        self.cmarkers_b = cmarkers_b\n",
        "        self.tmaps = tmaps\n",
        "        self.wmap_lengths = wmap_lengths\n",
        "        self.cmap_lengths = cmap_lengths\n",
        "\n",
        "        self.data_size = self.wmaps.size(0)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n",
        "               self.wmap_lengths[i], self.cmap_lengths[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LrptRggW3goj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "class ViterbiDecoder():\n",
        "    \"\"\"\n",
        "    Viterbi Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def decode(self, scores, lengths):\n",
        "        \"\"\"\n",
        "        :param scores: CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: decoded sequences\n",
        "        \"\"\"\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n",
        "\n",
        "        # Create a tensor to hold back-pointers\n",
        "        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n",
        "        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n",
        "        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n",
        "                                                               dtype=torch.long) * self.start_tag\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and\n",
        "                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n",
        "                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # Decode/trace best path backwards\n",
        "        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n",
        "        pointer = torch.ones((batch_size, 1),\n",
        "                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n",
        "\n",
        "        for t in list(reversed(range(backpointers.size(1)))):\n",
        "            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n",
        "            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n",
        "\n",
        "        # Sanity check\n",
        "        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n",
        "\n",
        "        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n",
        "        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n",
        "                            dim=1)\n",
        "\n",
        "        return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mWgvG2063jp0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Highway(nn.Module):\n",
        "    \"\"\"\n",
        "    Highway Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, num_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param size: size of linear layer (matches input size)\n",
        "        :param num_layers: number of transform and gate layers\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(Highway, self).__init__()\n",
        "        self.size = size\n",
        "        self.num_layers = num_layers\n",
        "        self.transform = nn.ModuleList()  # list of transform layers\n",
        "        self.gate = nn.ModuleList()  # list of gate layers\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        for i in range(num_layers):\n",
        "            transform = nn.Linear(size, size)\n",
        "            gate = nn.Linear(size, size)\n",
        "            self.transform.append(transform)\n",
        "            self.gate.append(gate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param x: input tensor\n",
        "        :return: output tensor, with same dimensions as input tensor\n",
        "        \"\"\"\n",
        "        transformed = nn.functional.relu(self.transform[0](x))  # transform input\n",
        "        g = nn.functional.sigmoid(self.gate[0](x))  # calculate how much of the transformed input to keep\n",
        "\n",
        "        out = g * transformed + (1 - g) * x  # combine input and transformed input in this ratio\n",
        "\n",
        "        # If there are additional layers\n",
        "        for i in range(1, self.num_layers):\n",
        "            out = self.dropout(out)\n",
        "            transformed = nn.functional.relu(self.transform[i](out))\n",
        "            g = nn.functional.sigmoid(self.gate[i](out))\n",
        "\n",
        "            out = g * transformed + (1 - g) * out\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    Conditional Random Field.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_dim, tagset_size):\n",
        "        \"\"\"\n",
        "        :param hidden_dim: size of word RNN/BLSTM's output\n",
        "        :param tagset_size: number of tags\n",
        "        \"\"\"\n",
        "        super(CRF, self).__init__()\n",
        "        self.tagset_size = tagset_size\n",
        "        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n",
        "        self.transition.data.zero_()\n",
        "\n",
        "    def forward(self, feats):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param feats: output of word RNN/BLSTM, a tensor of dimensions (batch_size, timesteps, hidden_dim)\n",
        "        :return: CRF scores, a tensor of dimensions (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        \"\"\"\n",
        "        self.batch_size = feats.size(0)\n",
        "        self.timesteps = feats.size(1)\n",
        "\n",
        "        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n",
        "        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size,\n",
        "                                                              self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "\n",
        "        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(\n",
        "            0)  # (batch_size, timesteps, tagset_size, tagset_size)\n",
        "        return crf_scores\n",
        "\n",
        "\n",
        "class LM_LSTM_CRF(nn.Module):\n",
        "    \"\"\"\n",
        "    The encompassing LM-LSTM-CRF model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n",
        "                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n",
        "        \"\"\"\n",
        "        :param tagset_size: number of tags\n",
        "        :param charset_size: size of character vocabulary\n",
        "        :param char_emb_dim: size of character embeddings\n",
        "        :param char_rnn_dim: size of character RNNs/LSTMs\n",
        "        :param char_rnn_layers: number of layers in character RNNs/LSTMs\n",
        "        :param vocab_size: input vocabulary size\n",
        "        :param lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n",
        "        :param word_emb_dim: size of word embeddings\n",
        "        :param word_rnn_dim: size of word RNN/BLSTM\n",
        "        :param word_rnn_layers:  number of layers in word RNNs/LSTMs\n",
        "        :param dropout: dropout\n",
        "        :param highway_layers: number of transform and gate layers\n",
        "        \"\"\"\n",
        "\n",
        "        super(LM_LSTM_CRF, self).__init__()\n",
        "\n",
        "        self.tagset_size = tagset_size  # this is the size of the output vocab of the tagging model\n",
        "\n",
        "        self.charset_size = charset_size\n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        self.char_rnn_dim = char_rnn_dim\n",
        "        self.char_rnn_layers = char_rnn_layers\n",
        "\n",
        "        self.wordset_size = vocab_size  # this is the size of the input vocab (embedding layer) of the tagging model\n",
        "        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n",
        "        self.word_emb_dim = word_emb_dim\n",
        "        self.word_rnn_dim = word_rnn_dim\n",
        "        self.word_rnn_layers = word_rnn_layers\n",
        "\n",
        "        self.highway_layers = highway_layers\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim)  # character embedding layer\n",
        "        self.forw_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n",
        "                                      bidirectional=False, dropout=dropout)  # forward character LSTM\n",
        "        self.back_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n",
        "                                      bidirectional=False, dropout=dropout)  # backward character LSTM\n",
        "\n",
        "        self.word_embeds = nn.Embedding(self.wordset_size, self.word_emb_dim)  # word embedding layer\n",
        "        self.word_blstm = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, self.word_rnn_dim // 2,\n",
        "                                  num_layers=self.word_rnn_layers, bidirectional=True, dropout=dropout)  # word BLSTM\n",
        "\n",
        "        self.crf = CRF((self.word_rnn_dim // 2) * 2, self.tagset_size)  # conditional random field\n",
        "\n",
        "        self.forw_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)  # highway to transform forward char LSTM output for the forward language model\n",
        "        self.back_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)  # highway to transform backward char LSTM output for the backward language model\n",
        "        self.subword_hw = Highway(2 * self.char_rnn_dim, num_layers=self.highway_layers,\n",
        "                                  dropout=dropout)  # highway to transform combined forward and backward char LSTM outputs for use in the word BLSTM\n",
        "\n",
        "        self.forw_lm_out = nn.Linear(self.char_rnn_dim,\n",
        "                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the forward language model\n",
        "        self.back_lm_out = nn.Linear(self.char_rnn_dim,\n",
        "                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the backward language model\n",
        "\n",
        "    def init_word_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Initialize embeddings with pre-trained embeddings.\n",
        "\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.word_embeds.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_word_embeddings(self, fine_tune=False):\n",
        "        \"\"\"\n",
        "        Fine-tune embedding layer? (Not fine-tuning only makes sense if using pre-trained embeddings).\n",
        "\n",
        "        :param fine_tune: Fine-tune?\n",
        "        \"\"\"\n",
        "        for p in self.word_embeds.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param cmaps_f: padded encoded forward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n",
        "        :param cmaps_b: padded encoded backward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n",
        "        :param cmarkers_f: padded forward character markers, a tensor of dimensions (batch_size, word_pad_len)\n",
        "        :param cmarkers_b: padded backward character markers, a tensor of dimensions (batch_size, word_pad_len)\n",
        "        :param wmaps: padded encoded word sequences, a tensor of dimensions (batch_size, word_pad_len)\n",
        "        :param tmaps: padded tag sequences, a tensor of dimensions (batch_size, word_pad_len)\n",
        "        :param wmap_lengths: word sequence lengths, a tensor of dimensions (batch_size)\n",
        "        :param cmap_lengths: character sequence lengths, a tensor of dimensions (batch_size, word_pad_len)\n",
        "        \"\"\"\n",
        "        self.batch_size = cmaps_f.size(0)\n",
        "        self.word_pad_len = wmaps.size(1)\n",
        "\n",
        "        # Sort by decreasing true char. sequence length\n",
        "        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n",
        "        cmaps_f = cmaps_f[char_sort_ind]\n",
        "        cmaps_b = cmaps_b[char_sort_ind]\n",
        "        cmarkers_f = cmarkers_f[char_sort_ind]\n",
        "        cmarkers_b = cmarkers_b[char_sort_ind]\n",
        "        wmaps = wmaps[char_sort_ind]\n",
        "        tmaps = tmaps[char_sort_ind]\n",
        "        wmap_lengths = wmap_lengths[char_sort_ind]\n",
        "\n",
        "        # Embedding look-up for characters\n",
        "        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.char_embeds(cmaps_b)\n",
        "\n",
        "        # Dropout\n",
        "        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n",
        "        cb = self.dropout(cb)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        cf = pack_padded_sequence(cf, cmap_lengths.tolist(),\n",
        "                                  batch_first=True)  # packed sequence of char_emb_dim, with real sequence lengths\n",
        "        cb = pack_padded_sequence(cb, cmap_lengths.tolist(), batch_first=True)\n",
        "\n",
        "        # LSTM\n",
        "        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n",
        "        cb, _ = self.back_char_lstm(cb)\n",
        "\n",
        "        # Unpack packed sequence\n",
        "        cf, _ = pad_packed_sequence(cf, batch_first=True)  # (batch_size, max_char_len_in_batch, char_rnn_dim)\n",
        "        cb, _ = pad_packed_sequence(cb, batch_first=True)\n",
        "\n",
        "        # Sanity check\n",
        "        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n",
        "\n",
        "        # Select RNN outputs only at marker points (spaces in the character sequence)\n",
        "        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n",
        "        cf_selected = torch.gather(cf, 1, cmarkers_f)  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "        cb_selected = torch.gather(cb, 1, cmarkers_b)\n",
        "\n",
        "        # Only for co-training, not useful for tagging after model is trained\n",
        "        if self.training:\n",
        "            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n",
        "            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n",
        "            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n",
        "            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n",
        "\n",
        "        # Sort by decreasing true word sequence length\n",
        "        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n",
        "        wmaps = wmaps[word_sort_ind]\n",
        "        tmaps = tmaps[word_sort_ind]\n",
        "        cf_selected = cf_selected[word_sort_ind]  # for language model\n",
        "        cb_selected = cb_selected[word_sort_ind]\n",
        "        if self.training:\n",
        "            lm_f_scores = lm_f_scores[word_sort_ind]\n",
        "            lm_b_scores = lm_b_scores[word_sort_ind]\n",
        "\n",
        "        # Embedding look-up for words\n",
        "        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        # Sub-word information at each word\n",
        "        subword = self.subword_hw(self.dropout(\n",
        "            torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n",
        "        subword = self.dropout(subword)\n",
        "\n",
        "        # Concatenate word embeddings and sub-word features\n",
        "        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, word_emb_dim + 2 * char_rnn_dim)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        w = pack_padded_sequence(w, list(wmap_lengths),\n",
        "                                 batch_first=True)  # packed sequence of word_emb_dim + 2 * char_rnn_dim, with real sequence lengths\n",
        "\n",
        "        # LSTM\n",
        "        w, _ = self.word_blstm(w)  # packed sequence of word_rnn_dim, with real sequence lengths\n",
        "\n",
        "        # Unpack packed sequence\n",
        "        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n",
        "        w = self.dropout(w)\n",
        "\n",
        "        crf_scores = self.crf(w)  # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n",
        "\n",
        "        if self.training:\n",
        "            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n",
        "        else:\n",
        "            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req.\n",
        "\n",
        "\n",
        "class ViterbiLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Viterbi Loss.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tag_map):\n",
        "        \"\"\"\n",
        "        :param tag_map: tag map\n",
        "        \"\"\"\n",
        "        super(ViterbiLoss, self).__init__()\n",
        "        self.tagset_size = len(tag_map)\n",
        "        self.start_tag = tag_map['<start>']\n",
        "        self.end_tag = tag_map['<end>']\n",
        "\n",
        "    def forward(self, scores, targets, lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "\n",
        "        :param scores: CRF scores\n",
        "        :param targets: true tags indices in unrolled CRF scores\n",
        "        :param lengths: word sequence lengths\n",
        "        :return: viterbi loss\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = scores.size(0)\n",
        "        word_pad_len = scores.size(1)\n",
        "\n",
        "        # Gold score\n",
        "\n",
        "        targets = targets.unsqueeze(2)\n",
        "        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n",
        "            2)  # (batch_size, word_pad_len)\n",
        "\n",
        "        # Everything is already sorted by lengths\n",
        "        scores_at_targets, _ = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n",
        "        gold_score = scores_at_targets.sum()\n",
        "\n",
        "        # All paths' scores\n",
        "\n",
        "        # Create a tensor to hold accumulated sequence scores at each current tag\n",
        "        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n",
        "\n",
        "        for t in range(max(lengths)):\n",
        "            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n",
        "            if t == 0:\n",
        "                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n",
        "            else:\n",
        "                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n",
        "                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n",
        "                # So, broadcast prev. timestep's cur_tag scores along cur. timestep's cur_tag dimension\n",
        "                scores_upto_t[:batch_size_t] = log_sum_exp(\n",
        "                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n",
        "                    dim=1)  # (batch_size, tagset_size)\n",
        "\n",
        "        # We only need the final accumulated scores at the <end> tag\n",
        "        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n",
        "\n",
        "        viterbi_loss = all_paths_scores - gold_score\n",
        "        viterbi_loss = viterbi_loss / batch_size\n",
        "\n",
        "        return viterbi_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDn2xf-54_Xv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import codecs\n",
        "import itertools\n",
        "from functools import reduce\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "\n",
        "def read_words_tags(file, tag_ind, caseless=False):\n",
        "    \"\"\"\n",
        "    Reads raw data in the CoNLL 2003 format and returns word and tag sequences.\n",
        "\n",
        "    :param file: file with raw data in the CoNLL 2003 format\n",
        "    :param tag_ind: column index of tag\n",
        "    :param caseless: lowercase words?\n",
        "    :return: word, tag sequences\n",
        "    \"\"\"\n",
        "    with codecs.open(file, 'r', 'utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    words = []\n",
        "    tags = []\n",
        "    temp_w = []\n",
        "    temp_t = []\n",
        "    for line in lines:\n",
        "        if not (line.isspace() or (len(line) > 10 and line[0:10] == '-DOCSTART-')):\n",
        "            feats = line.rstrip('\\n').split()\n",
        "            if len(feats)<2:\n",
        "                continue\n",
        "            temp_w.append(feats[0].lower() if caseless else feats[0])\n",
        "            temp_t.append(feats[tag_ind])\n",
        "        elif len(temp_w) > 0:\n",
        "            assert len(temp_w) == len(temp_t)\n",
        "            words.append(temp_w)\n",
        "            tags.append(temp_t)\n",
        "            temp_w = []\n",
        "            temp_t = []\n",
        "    # last sentence\n",
        "    if len(temp_w) > 0:\n",
        "        assert len(temp_w) == len(temp_t)\n",
        "        words.append(temp_w)\n",
        "        tags.append(temp_t)\n",
        "\n",
        "    # Sanity check\n",
        "    assert len(words) == len(tags)\n",
        "\n",
        "    return words, tags\n",
        "\n",
        "\n",
        "def create_maps(words, tags, min_word_freq=2, min_char_freq=1):\n",
        "    \"\"\"\n",
        "    Creates word, char, tag maps.\n",
        "\n",
        "    :param words: word sequences\n",
        "    :param tags: tag sequences\n",
        "    :param min_word_freq: words that occur fewer times than this threshold are binned as <unk>s\n",
        "    :param min_char_freq: characters that occur fewer times than this threshold are binned as <unk>s\n",
        "    :return: word, char, tag maps\n",
        "    \"\"\"\n",
        "    word_freq = Counter()\n",
        "    char_freq = Counter()\n",
        "    tag_map = set()\n",
        "    for w, t in zip(words, tags):\n",
        "        word_freq.update(w)\n",
        "        char_freq.update(list(reduce(lambda x, y: list(x) + [' '] + list(y), w)))\n",
        "        tag_map.update(t)\n",
        "\n",
        "    word_map = {k.lower(): v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n",
        "    char_map = {k: v + 1 for v, k in enumerate([c for c in char_freq.keys() if char_freq[c] > min_char_freq])}\n",
        "    tag_map = {k: v + 1 for v, k in enumerate(tag_map)}\n",
        "\n",
        "    word_map['<pad>'] = 0\n",
        "    word_map['<end>'] = len(word_map)\n",
        "    word_map['<unk>'] = len(word_map)\n",
        "    char_map['<pad>'] = 0\n",
        "    char_map['<end>'] = len(char_map)\n",
        "    char_map['<unk>'] = len(char_map)\n",
        "    tag_map['<pad>'] = 0\n",
        "    tag_map['<start>'] = len(tag_map)\n",
        "    tag_map['<end>'] = len(tag_map)\n",
        "\n",
        "    return word_map, char_map, tag_map\n",
        "\n",
        "\n",
        "def create_input_tensors(words, tags, word_map, char_map, tag_map):\n",
        "    \"\"\"\n",
        "    Creates input tensors that will be used to create a PyTorch Dataset.\n",
        "\n",
        "    :param words: word sequences\n",
        "    :param tags: tag sequences\n",
        "    :param word_map: word map\n",
        "    :param char_map: character map\n",
        "    :param tag_map: tag map\n",
        "    :return: padded encoded words, padded encoded forward chars, padded encoded backward chars,\n",
        "            padded forward character markers, padded backward character markers, padded encoded tags,\n",
        "            word sequence lengths, char sequence lengths\n",
        "    \"\"\"\n",
        "    # Encode sentences into word maps with <end> at the end\n",
        "    # [['dunston', 'checks', 'in', '<end>']] -> [[4670, 4670, 185, 4669]]\n",
        "    wmaps = list(map(lambda s: list(map(lambda w: word_map.get(w.lower(), word_map['<unk>']), s)) + [word_map['<end>']], words))\n",
        "\n",
        "    # Forward and backward character streams\n",
        "    # [['d', 'u', 'n', 's', 't', 'o', 'n', ' ', 'c', 'h', 'e', 'c', 'k', 's', ' ', 'i', 'n', ' ']]\n",
        "    chars_f = list(map(lambda s: list(reduce(lambda x, y: list(x) + [' '] + list(y), s)) + [' '], words))\n",
        "    # [['n', 'i', ' ', 's', 'k', 'c', 'e', 'h', 'c', ' ', 'n', 'o', 't', 's', 'n', 'u', 'd', ' ']]\n",
        "    chars_b = list(\n",
        "        map(lambda s: list(reversed([' '] + list(reduce(lambda x, y: list(x) + [' '] + list(y), s)))), words))\n",
        "\n",
        "    # Encode streams into forward and backward character maps with <end> at the end\n",
        "    # [[29, 2, 12, 8, 7, 14, 12, 3, 6, 18, 1, 6, 21, 8, 3, 17, 12, 3, 60]]\n",
        "    cmaps_f = list(\n",
        "        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_f))\n",
        "    # [[12, 17, 3, 8, 21, 6, 1, 18, 6, 3, 12, 14, 7, 8, 12, 2, 29, 3, 60]]\n",
        "    cmaps_b = list(\n",
        "        map(lambda s: list(map(lambda c: char_map.get(c, char_map['<unk>']), s)) + [char_map['<end>']], chars_b))\n",
        "\n",
        "    # Positions of spaces and <end> character\n",
        "    # Words are predicted or encoded at these places in the language and tagging models respectively\n",
        "    # [[7, 14, 17, 18]] are points after '...dunston', '...checks', '...in', '...<end>' respectively\n",
        "    cmarkers_f = list(map(lambda s: [ind for ind in range(len(s)) if s[ind] == char_map[' ']] + [len(s) - 1], cmaps_f))\n",
        "    # Reverse the markers for the backward stream before adding <end>, so the words of the f and b markers coincide\n",
        "    # i.e., [[17, 9, 2, 18]] are points after '...notsnud', '...skcehc', '...ni', '...<end>' respectively\n",
        "    cmarkers_b = list(\n",
        "        map(lambda s: list(reversed([ind for ind in range(len(s)) if s[ind] == char_map[' ']])) + [len(s) - 1],\n",
        "            cmaps_b))\n",
        "\n",
        "    # Encode tags into tag maps with <end> at the end\n",
        "    tmaps = list(map(lambda s: list(map(lambda t: tag_map[t], s)) + [tag_map['<end>']], tags))\n",
        "    # Since we're using CRF scores of size (prev_tags, cur_tags), find indices of target sequence in the unrolled scores\n",
        "    # This will be row_index (i.e. prev_tag) * n_columns (i.e. tagset_size) + column_index (i.e. cur_tag)\n",
        "    tmaps = list(map(lambda s: [tag_map['<start>'] * len(tag_map) + s[0]] + [s[i - 1] * len(tag_map) + s[i] for i in\n",
        "                                                                             range(1, len(s))], tmaps))\n",
        "    # Note - the actual tag indices can be recovered with tmaps % len(tag_map)\n",
        "\n",
        "    # Pad, because need fixed length to be passed around by DataLoaders and other layers\n",
        "    word_pad_len = max(list(map(lambda s: len(s), wmaps)))\n",
        "    char_pad_len = max(list(map(lambda s: len(s), cmaps_f)))\n",
        "\n",
        "    # Sanity check\n",
        "    assert word_pad_len == max(list(map(lambda s: len(s), tmaps)))\n",
        "\n",
        "    padded_wmaps = []\n",
        "    padded_cmaps_f = []\n",
        "    padded_cmaps_b = []\n",
        "    padded_cmarkers_f = []\n",
        "    padded_cmarkers_b = []\n",
        "    padded_tmaps = []\n",
        "    wmap_lengths = []\n",
        "    cmap_lengths = []\n",
        "\n",
        "    for w, cf, cb, cmf, cmb, t in zip(wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps):\n",
        "        # Sanity  checks\n",
        "        assert len(w) == len(cmf) == len(cmb) == len(t)\n",
        "        assert len(cmaps_f) == len(cmaps_b)\n",
        "\n",
        "        # Pad\n",
        "        # A note -  it doesn't really matter what we pad with, as long as it's a valid index\n",
        "        # i.e., we'll extract output at those pad points (to extract equal lengths), but never use them\n",
        "\n",
        "        padded_wmaps.append(w + [word_map['<pad>']] * (word_pad_len - len(w)))\n",
        "        padded_cmaps_f.append(cf + [char_map['<pad>']] * (char_pad_len - len(cf)))\n",
        "        padded_cmaps_b.append(cb + [char_map['<pad>']] * (char_pad_len - len(cb)))\n",
        "\n",
        "        # 0 is always a valid index to pad markers with (-1 is too but torch.gather has some issues with it)\n",
        "        padded_cmarkers_f.append(cmf + [0] * (word_pad_len - len(w)))\n",
        "        padded_cmarkers_b.append(cmb + [0] * (word_pad_len - len(w)))\n",
        "\n",
        "        padded_tmaps.append(t + [tag_map['<pad>']] * (word_pad_len - len(t)))\n",
        "\n",
        "        wmap_lengths.append(len(w))\n",
        "        cmap_lengths.append(len(cf))\n",
        "\n",
        "        # Sanity check\n",
        "        assert len(padded_wmaps[-1]) == len(padded_tmaps[-1]) == len(padded_cmarkers_f[-1]) == len(\n",
        "            padded_cmarkers_b[-1]) == word_pad_len\n",
        "        assert len(padded_cmaps_f[-1]) == len(padded_cmaps_b[-1]) == char_pad_len\n",
        "\n",
        "    padded_wmaps = torch.LongTensor(padded_wmaps)\n",
        "    padded_cmaps_f = torch.LongTensor(padded_cmaps_f)\n",
        "    padded_cmaps_b = torch.LongTensor(padded_cmaps_b)\n",
        "    padded_cmarkers_f = torch.LongTensor(padded_cmarkers_f)\n",
        "    padded_cmarkers_b = torch.LongTensor(padded_cmarkers_b)\n",
        "    padded_tmaps = torch.LongTensor(padded_tmaps)\n",
        "    wmap_lengths = torch.LongTensor(wmap_lengths)\n",
        "    cmap_lengths = torch.LongTensor(cmap_lengths)\n",
        "\n",
        "    return padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, \\\n",
        "           wmap_lengths, cmap_lengths\n",
        "\n",
        "\n",
        "def init_embedding(input_embedding):\n",
        "    \"\"\"\n",
        "    Initialize embedding tensor with values from the uniform distribution.\n",
        "\n",
        "    :param input_embedding: embedding tensor\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "    nn.init.uniform_(input_embedding, -bias, bias)\n",
        "\n",
        "\n",
        "def load_embeddings(word_map, expand_vocab=True):\n",
        "    \"\"\"\n",
        "    Load pre-trained embeddings for words in the word map.\n",
        "\n",
        "    :param emb_file: file with pre-trained embeddings (in the GloVe format)\n",
        "    :param word_map: word map\n",
        "    :param expand_vocab: expand vocabulary of word map to vocabulary of pre-trained embeddings?\n",
        "    :return: embeddings for words in word map, (possibly expanded) word map,\n",
        "            number of words in word map that are in-corpus (subject to word frequency threshold)\n",
        "    \"\"\"\n",
        "  \n",
        "\n",
        "    # Create tensor to hold embeddings for words that are in-corpus\n",
        "    emb_len=100\n",
        "    ic_embs = torch.FloatTensor(len(word_map), emb_len)\n",
        "    init_embedding(ic_embs)\n",
        "\n",
        "    if expand_vocab:\n",
        "        print(\"You have elected to include embeddings that are out-of-corpus.\")\n",
        "        ooc_words = []\n",
        "        ooc_embs = []\n",
        "    else:\n",
        "        print(\"You have elected NOT to include embeddings that are out-of-corpus.\")\n",
        "\n",
        "\n",
        "    lm_vocab_size = len(word_map)  # keep track of lang. model's output vocab size (no out-of-corpus words)\n",
        "\n",
        "    embeddings = ic_embs\n",
        "\n",
        "    # Sanity check\n",
        "    assert embeddings.size(0) == len(word_map)\n",
        "\n",
        "    print(\"\\nDone.\\n Embedding vocabulary: %d\\n Language Model vocabulary: %d.\\n\" % (len(word_map), lm_vocab_size))\n",
        "\n",
        "    return embeddings, word_map, lm_vocab_size\n",
        "\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clip gradients computed during backpropagation to prevent gradient explosion.\n",
        "\n",
        "    :param optimizer: optimized with the gradients to be clipped\n",
        "    :param grad_clip: gradient clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "\n",
        "def save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best):\n",
        "    \"\"\"\n",
        "    Save model checkpoint.\n",
        "\n",
        "    :param epoch: epoch number\n",
        "    :param model: model\n",
        "    :param optimizer: optimized\n",
        "    :param val_f1: validation F1 score\n",
        "    :param word_map: word map\n",
        "    :param char_map: char map\n",
        "    :param tag_map: tag map\n",
        "    :param lm_vocab_size: number of words in-corpus, i.e. size of output vocabulary of linear model\n",
        "    :param is_best: is this checkpoint the best so far?\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'f1': val_f1,\n",
        "             'model': model,\n",
        "             'optimizer': optimizer,\n",
        "             'word_map': word_map,\n",
        "             'tag_map': tag_map,\n",
        "             'char_map': char_map,\n",
        "             'lm_vocab_size': lm_vocab_size}\n",
        "    filename = 'ner-vi.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    # If checkpoint is the best so far, create a copy to avoid being overwritten by a subsequent worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, '/content/drive/My Drive/nlp/BEST_' + filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, new_lr):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "\n",
        "    :param optimizer: optimizer whose learning rates must be decayed\n",
        "    :param new_lr: new learning rate\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = new_lr\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def log_sum_exp(tensor, dim):\n",
        "    \"\"\"\n",
        "    Calculates the log-sum-exponent of a tensor's dimension in a numerically stable way.\n",
        "\n",
        "    :param tensor: tensor\n",
        "    :param dim: dimension to calculate log-sum-exp of\n",
        "    :return: log-sum-exp\n",
        "    \"\"\"\n",
        "    m, _ = torch.max(tensor, dim)\n",
        "    m_expanded = m.unsqueeze(dim).expand_as(tensor)\n",
        "    return m + torch.log(torch.sum(torch.exp(tensor - m_expanded), dim))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DFnceSSD4DtY",
        "colab_type": "code",
        "outputId": "3d6c4f97-d20d-4af1-f7ae-ba8c8da9226b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2914
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Data parameters\n",
        "task = 'ner'  # tagging task, to choose column in CoNLL 2003 dataset\n",
        "train_file = '/content/drive/My Drive/nlp/train-filter.txt'  # path to training data\n",
        "val_file = '/content/drive/My Drive/nlp/test-a-filter.txt'  # path to validation data\n",
        "test_file = '/content/drive/My Drive/nlp/test-b-filter.txt'  # path to test data\n",
        "\n",
        "min_word_freq = 5  # threshold for word frequency\n",
        "min_char_freq = 1  # threshold for character frequency\n",
        "caseless = False  # lowercase everything?\n",
        "expand_vocab = True  # expand model's input vocabulary to the pre-trained embeddings' vocabulary?\n",
        "\n",
        "# Model parameters\n",
        "char_emb_dim = 30  # character embedding size\n",
        "word_emb_dim = 100 # word embedding size\n",
        "word_rnn_dim = 300  # word RNN size\n",
        "char_rnn_dim = 300  # character RNN size\n",
        "char_rnn_layers = 1  # number of layers in character RNN\n",
        "word_rnn_layers = 1  # number of layers in word RNN\n",
        "highway_layers = 1  # number of layers in highway network\n",
        "dropout = 0.5  # dropout\n",
        "fine_tune_word_embeddings = True  # fine-tune pre-trained word embeddings?\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0  # start at this epoch\n",
        "batch_size = 40  # batch size\n",
        "lr = 0.015  # learning rate\n",
        "lr_decay = 0.05  # decay learning rate by this amount\n",
        "momentum = 0.9  # momentum\n",
        "workers = 1  # number of workers for loading data in the DataLoader\n",
        "epochs = 200  # number of epochs to run without early-stopping\n",
        "grad_clip = 5.  # clip gradients at this value\n",
        "print_freq = 100  # print training or validation status every __ batches\n",
        "best_f1 = 0.  # F1 score to start with\n",
        "checkpoint =  '/content/drive/My Drive/nlp/BEST_ner-vi.pth.tar'  # path to model checkpoint, None if none\n",
        "\n",
        "epochs_since_improvement = 0\n",
        "tag_ind = 1   # choose column in CoNLL 2003 dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Training and validation.\n",
        "    \"\"\"\n",
        "    global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n",
        "\n",
        "    # Read training and validation data\n",
        "    train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n",
        "    val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n",
        "\n",
        "    # Initialize model or load checkpoint\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        word_map = checkpoint['word_map']\n",
        "        lm_vocab_size = checkpoint['lm_vocab_size']\n",
        "        tag_map = checkpoint['tag_map']\n",
        "        char_map = checkpoint['char_map']\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        best_f1 = checkpoint['f1']\n",
        "    except:\n",
        "        word_map, char_map, tag_map = create_maps(train_words + val_words, train_tags + val_tags, min_word_freq,\n",
        "                                                  min_char_freq)  # create word, char, tag maps\n",
        "        embeddings, word_map, lm_vocab_size = load_embeddings(word_map,\n",
        "                                                              expand_vocab)  # load pre-trained embeddings\n",
        "\n",
        "        \n",
        "        model = LM_LSTM_CRF(tagset_size=len(tag_map),\n",
        "                            charset_size=len(char_map),\n",
        "                            char_emb_dim=char_emb_dim,\n",
        "                            char_rnn_dim=char_rnn_dim,\n",
        "                            char_rnn_layers=char_rnn_layers,\n",
        "                            vocab_size=len(word_map),\n",
        "                            lm_vocab_size=lm_vocab_size,\n",
        "                            word_emb_dim=word_emb_dim,\n",
        "                            word_rnn_dim=word_rnn_dim,\n",
        "                            word_rnn_layers=word_rnn_layers,\n",
        "                            dropout=dropout,\n",
        "                            highway_layers=highway_layers).to(device)\n",
        "        #model.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\n",
        "        #model.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\n",
        "        optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n",
        "    #print(word_map)\n",
        "    #print(char_map)\n",
        "    print(tag_map)\n",
        "    # Loss functions\n",
        "    lm_criterion = nn.CrossEntropyLoss().to(device)\n",
        "    crf_criterion = ViterbiLoss(tag_map).to(device)\n",
        "\n",
        "    # Since the language model's vocab is restricted to in-corpus indices, encode training/val with only these!\n",
        "    # word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n",
        "    # they were in the pre-trained embeddings\n",
        "    temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
        "    train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n",
        "                                        tag_map)\n",
        "    val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                               num_workers=workers, pin_memory=False)\n",
        "    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
        "                                             num_workers=workers, pin_memory=False)\n",
        "\n",
        "    # Viterbi decoder (to find accuracy during validation)\n",
        "    vb_decoder = ViterbiDecoder(tag_map)\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              model=model,\n",
        "              lm_criterion=lm_criterion,\n",
        "              crf_criterion=crf_criterion,\n",
        "              optimizer=optimizer,\n",
        "              epoch=epoch,\n",
        "              vb_decoder=vb_decoder)\n",
        "\n",
        "        # One epoch's validation\n",
        "        val_f1 = validate(val_loader=val_loader,\n",
        "                          model=model,\n",
        "                          crf_criterion=crf_criterion,\n",
        "                          vb_decoder=vb_decoder)\n",
        "\n",
        "        # Did validation F1 score improve?\n",
        "        is_best = val_f1 > best_f1\n",
        "        best_f1 = max(val_f1, best_f1)\n",
        "        \n",
        "        if not is_best:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "        else:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n",
        "\n",
        "        # Decay learning rate every epoch\n",
        "        adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))\n",
        "\n",
        "\n",
        "def train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param model: model\n",
        "    :param lm_criterion: cross entropy loss layer\n",
        "    :param crf_criterion: viterbi loss layer\n",
        "    :param optimizer: optimizer\n",
        "    :param epoch: epoch number\n",
        "    :param vb_decoder: viterbi decoder (to decode and find F1 score)\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()  # training mode enables dropout\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n",
        "    data_time = AverageMeter()  # data loading time per batch\n",
        "    ce_losses = AverageMeter()  # cross entropy loss\n",
        "    vb_losses = AverageMeter()  # viterbi loss\n",
        "    f1s = AverageMeter()  # f1 score\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
        "            train_loader):\n",
        "\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                                             cmaps_b,\n",
        "                                                                                                             cmarkers_f,\n",
        "                                                                                                             cmarkers_b,\n",
        "                                                                                                             wmaps,\n",
        "                                                                                                             tmaps,\n",
        "                                                                                                             wmap_lengths,\n",
        "                                                                                                             cmap_lengths)\n",
        "\n",
        "        # LM loss\n",
        "\n",
        "        # We don't predict the next word at the pads or <end> tokens\n",
        "        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n",
        "        # So, prediction lengths are word sequence lengths - 1\n",
        "        lm_lengths = wmap_lengths_sorted - 1\n",
        "        lm_lengths = lm_lengths.tolist()\n",
        "\n",
        "        # Remove scores at timesteps we won't predict at\n",
        "        # pack_padded_sequence is a good trick to do this (see dynamic_rnn.py, where we explore this)\n",
        "        lm_f_scores, _ = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n",
        "        lm_b_scores, _ = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the forward sequence, targets are from the second word onwards, up to <end>\n",
        "        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n",
        "        lm_f_targets = wmaps_sorted[:, 1:]\n",
        "        lm_f_targets, _ = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # For the backward sequence, targets are <end> followed by all words except the last word\n",
        "        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n",
        "        lm_b_targets = torch.cat(\n",
        "            [torch.LongTensor([word_map['<end>']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), wmaps_sorted], dim=1)\n",
        "        lm_b_targets, _ = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        ce_loss = lm_criterion(lm_f_scores, lm_f_targets) + lm_criterion(lm_b_scores, lm_b_targets)\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "        loss = ce_loss + vb_loss\n",
        "\n",
        "        # Back prop.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(optimizer, grad_clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded, _ = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n",
        "\n",
        "        # F1\n",
        "        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        batch_time.update(time.time() - start)\n",
        "        f1s.update(f1, sum(lm_lengths))\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print training status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 {f1.val:.3f} ({f1.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                          batch_time=batch_time,\n",
        "                                                          data_time=data_time, ce_loss=ce_losses,\n",
        "                                                          vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "\n",
        "def validate(val_loader, model, crf_criterion, vb_decoder):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "\n",
        "    :param val_loader: DataLoader for validation data\n",
        "    :param model: model\n",
        "    :param crf_criterion: viterbi loss layer\n",
        "    :param vb_decoder: viterbi decoder\n",
        "    :return: validation F1 score\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    vb_losses = AverageMeter()\n",
        "    f1s = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
        "            val_loader):\n",
        "\n",
        "        max_word_len = max(wmap_lengths.tolist())\n",
        "        max_char_len = max(cmap_lengths.tolist())\n",
        "\n",
        "        # Reduce batch's padded length to maximum in-batch sequence\n",
        "        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don't compute over the pads)\n",
        "        wmaps = wmaps[:, :max_word_len].to(device)\n",
        "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
        "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
        "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
        "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
        "        tmaps = tmaps[:, :max_word_len].to(device)\n",
        "        wmap_lengths = wmap_lengths.to(device)\n",
        "        cmap_lengths = cmap_lengths.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
        "                                                                                   cmaps_b,\n",
        "                                                                                   cmarkers_f,\n",
        "                                                                                   cmarkers_b,\n",
        "                                                                                   wmaps,\n",
        "                                                                                   tmaps,\n",
        "                                                                                   wmap_lengths,\n",
        "                                                                                   cmap_lengths)\n",
        "\n",
        "        # Viterbi / CRF layer loss\n",
        "        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n",
        "\n",
        "        # Viterbi decode to find accuracy / f1\n",
        "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
        "\n",
        "        # Remove timesteps we won't predict at, and also <end> tags, because to predict them would be cheating\n",
        "        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n",
        "        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
        "\n",
        "        # f1\n",
        "        f1 = f1_score(tmaps_sorted.to(\"cpu\").numpy(), decoded.numpy(), average='macro')\n",
        "\n",
        "        # Keep track of metrics\n",
        "        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n",
        "        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        if i % print_freq == 0:\n",
        "            print('Validation: [{0}/{1}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t'\n",
        "                  'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                  vb_loss=vb_losses, f1=f1s))\n",
        "\n",
        "    print(\n",
        "        '\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n'.format(vb_loss=vb_losses,\n",
        "                                                                          f1=f1s))\n",
        "\n",
        "    return f1s.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have elected to include embeddings that are out-of-corpus.\n",
            "\n",
            "Done.\n",
            " Embedding vocabulary: 8566\n",
            " Language Model vocabulary: 8566.\n",
            "\n",
            "{'O': 1, 'I-PER': 2, 'I-MISC': 3, 'I-ORG': 4, 'B-ORG': 5, 'B-LOC': 6, 'B-PER': 7, 'I-LOC': 8, 'B-MISC': 9, '<pad>': 0, '<start>': 10, '<end>': 11}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/2539]\tBatch Time 1.933 (1.933)\tData Load Time 0.122 (0.122)\tCE Loss 18.1204 (18.1204)\tVB Loss 84.9608 (84.9608)\tF1 0.034 (0.034)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: [0][100/2539]\tBatch Time 1.368 (1.337)\tData Load Time 0.007 (0.008)\tCE Loss 17.8473 (17.9898)\tVB Loss 7.5844 (12.7323)\tF1 0.275 (0.150)\n",
            "Epoch: [0][200/2539]\tBatch Time 1.441 (1.338)\tData Load Time 0.007 (0.008)\tCE Loss 16.4615 (17.6289)\tVB Loss 5.3831 (9.6355)\tF1 0.315 (0.225)\n",
            "Epoch: [0][300/2539]\tBatch Time 1.512 (1.349)\tData Load Time 0.007 (0.008)\tCE Loss 14.9825 (17.0104)\tVB Loss 4.0035 (8.1940)\tF1 0.530 (0.288)\n",
            "Epoch: [0][400/2539]\tBatch Time 1.652 (1.356)\tData Load Time 0.007 (0.007)\tCE Loss 14.0326 (16.3487)\tVB Loss 4.2430 (7.2979)\tF1 0.443 (0.333)\n",
            "Epoch: [0][500/2539]\tBatch Time 1.202 (1.353)\tData Load Time 0.007 (0.007)\tCE Loss 13.7839 (15.8542)\tVB Loss 4.1252 (6.6220)\tF1 0.505 (0.367)\n",
            "Epoch: [0][600/2539]\tBatch Time 1.169 (1.348)\tData Load Time 0.007 (0.007)\tCE Loss 13.5953 (15.4924)\tVB Loss 2.7354 (6.0925)\tF1 0.583 (0.396)\n",
            "Epoch: [0][700/2539]\tBatch Time 1.235 (1.352)\tData Load Time 0.007 (0.007)\tCE Loss 13.3901 (15.2130)\tVB Loss 2.4064 (5.6868)\tF1 0.697 (0.422)\n",
            "Epoch: [0][800/2539]\tBatch Time 1.696 (1.356)\tData Load Time 0.007 (0.007)\tCE Loss 13.4867 (14.9964)\tVB Loss 4.7098 (5.3620)\tF1 0.493 (0.442)\n",
            "Epoch: [0][900/2539]\tBatch Time 1.605 (1.356)\tData Load Time 0.007 (0.007)\tCE Loss 13.4584 (14.8234)\tVB Loss 2.9641 (5.0719)\tF1 0.681 (0.463)\n",
            "Epoch: [0][1000/2539]\tBatch Time 1.531 (1.352)\tData Load Time 0.007 (0.007)\tCE Loss 13.7265 (14.6828)\tVB Loss 2.3082 (4.8390)\tF1 0.681 (0.480)\n",
            "Epoch: [0][1100/2539]\tBatch Time 1.580 (1.353)\tData Load Time 0.007 (0.007)\tCE Loss 13.5776 (14.5620)\tVB Loss 2.1156 (4.6389)\tF1 0.587 (0.494)\n",
            "Epoch: [0][1200/2539]\tBatch Time 1.524 (1.353)\tData Load Time 0.007 (0.007)\tCE Loss 13.3350 (14.4607)\tVB Loss 3.2741 (4.4633)\tF1 0.650 (0.508)\n",
            "Epoch: [0][1300/2539]\tBatch Time 1.110 (1.350)\tData Load Time 0.006 (0.007)\tCE Loss 13.2265 (14.3714)\tVB Loss 1.7976 (4.3108)\tF1 0.826 (0.521)\n",
            "Epoch: [0][1400/2539]\tBatch Time 1.540 (1.348)\tData Load Time 0.007 (0.007)\tCE Loss 13.3506 (14.2950)\tVB Loss 2.2336 (4.1736)\tF1 0.616 (0.530)\n",
            "Epoch: [0][1500/2539]\tBatch Time 1.403 (1.348)\tData Load Time 0.007 (0.007)\tCE Loss 13.3867 (14.2272)\tVB Loss 1.5702 (4.0528)\tF1 0.744 (0.539)\n",
            "Epoch: [0][1600/2539]\tBatch Time 1.529 (1.346)\tData Load Time 0.007 (0.007)\tCE Loss 13.3905 (14.1666)\tVB Loss 2.8578 (3.9404)\tF1 0.580 (0.549)\n",
            "Epoch: [0][1700/2539]\tBatch Time 1.510 (1.346)\tData Load Time 0.006 (0.007)\tCE Loss 13.1865 (14.1120)\tVB Loss 1.7436 (3.8402)\tF1 0.696 (0.557)\n",
            "Epoch: [0][1800/2539]\tBatch Time 1.686 (1.346)\tData Load Time 0.007 (0.007)\tCE Loss 13.0389 (14.0630)\tVB Loss 1.7973 (3.7453)\tF1 0.559 (0.565)\n",
            "Epoch: [0][1900/2539]\tBatch Time 1.265 (1.343)\tData Load Time 0.007 (0.007)\tCE Loss 13.4102 (14.0183)\tVB Loss 2.7988 (3.6605)\tF1 0.761 (0.572)\n",
            "Epoch: [0][2000/2539]\tBatch Time 1.623 (1.344)\tData Load Time 0.007 (0.007)\tCE Loss 13.3554 (13.9767)\tVB Loss 1.4411 (3.5842)\tF1 0.787 (0.579)\n",
            "Epoch: [0][2100/2539]\tBatch Time 1.693 (1.344)\tData Load Time 0.007 (0.007)\tCE Loss 13.1633 (13.9400)\tVB Loss 2.6293 (3.5080)\tF1 0.616 (0.585)\n",
            "Epoch: [0][2200/2539]\tBatch Time 1.212 (1.341)\tData Load Time 0.007 (0.007)\tCE Loss 13.3831 (13.9064)\tVB Loss 2.2246 (3.4379)\tF1 0.730 (0.592)\n",
            "Epoch: [0][2300/2539]\tBatch Time 1.403 (1.342)\tData Load Time 0.007 (0.007)\tCE Loss 13.3688 (13.8750)\tVB Loss 1.8697 (3.3748)\tF1 0.768 (0.597)\n",
            "Epoch: [0][2400/2539]\tBatch Time 1.616 (1.344)\tData Load Time 0.007 (0.007)\tCE Loss 13.1397 (13.8454)\tVB Loss 1.2684 (3.3179)\tF1 0.745 (0.602)\n",
            "Epoch: [0][2500/2539]\tBatch Time 1.581 (1.343)\tData Load Time 0.007 (0.007)\tCE Loss 13.4505 (13.8181)\tVB Loss 2.3309 (3.2623)\tF1 0.776 (0.606)\n",
            "Validation: [0/317]\tBatch Time 0.771 (0.771)\tVB Loss 1.0932 (1.0932)\tF1 Score 0.877 (0.877)\t\n",
            "Validation: [100/317]\tBatch Time 0.537 (0.580)\tVB Loss 2.1580 (1.4504)\tF1 Score 0.774 (0.805)\t\n",
            "Validation: [200/317]\tBatch Time 0.667 (0.578)\tVB Loss 1.3581 (1.4342)\tF1 Score 0.853 (0.806)\t\n",
            "Validation: [300/317]\tBatch Time 0.717 (0.571)\tVB Loss 1.9920 (1.4336)\tF1 Score 0.778 (0.801)\t\n",
            "\n",
            " * LOSS - 1.431, F1 SCORE - 0.801\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type LM_LSTM_CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type CRF. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Highway. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.014286\n",
            "\n",
            "Epoch: [1][0/2539]\tBatch Time 1.836 (1.836)\tData Load Time 0.134 (0.134)\tCE Loss 13.1166 (13.1166)\tVB Loss 1.3215 (1.3215)\tF1 0.890 (0.890)\n",
            "Epoch: [1][100/2539]\tBatch Time 1.241 (1.362)\tData Load Time 0.007 (0.008)\tCE Loss 13.0693 (13.1786)\tVB Loss 1.6257 (1.8765)\tF1 0.760 (0.726)\n",
            "Epoch: [1][200/2539]\tBatch Time 1.517 (1.353)\tData Load Time 0.007 (0.008)\tCE Loss 13.1062 (13.1600)\tVB Loss 1.5009 (1.8614)\tF1 0.771 (0.727)\n",
            "Epoch: [1][300/2539]\tBatch Time 0.939 (1.343)\tData Load Time 0.006 (0.007)\tCE Loss 13.1668 (13.1596)\tVB Loss 1.5147 (1.8462)\tF1 0.734 (0.728)\n",
            "Epoch: [1][400/2539]\tBatch Time 1.187 (1.328)\tData Load Time 0.007 (0.007)\tCE Loss 13.1191 (13.1517)\tVB Loss 1.8396 (1.8426)\tF1 0.839 (0.728)\n",
            "Epoch: [1][500/2539]\tBatch Time 1.249 (1.331)\tData Load Time 0.006 (0.007)\tCE Loss 13.0122 (13.1471)\tVB Loss 1.8659 (1.8344)\tF1 0.758 (0.727)\n",
            "Epoch: [1][600/2539]\tBatch Time 1.736 (1.325)\tData Load Time 0.006 (0.007)\tCE Loss 13.0900 (13.1428)\tVB Loss 1.3890 (1.8242)\tF1 0.752 (0.727)\n",
            "Epoch: [1][700/2539]\tBatch Time 1.641 (1.323)\tData Load Time 0.007 (0.007)\tCE Loss 12.9919 (13.1379)\tVB Loss 2.5164 (1.8207)\tF1 0.579 (0.727)\n",
            "Epoch: [1][800/2539]\tBatch Time 1.277 (1.321)\tData Load Time 0.007 (0.007)\tCE Loss 13.2407 (13.1322)\tVB Loss 1.8138 (1.8153)\tF1 0.871 (0.730)\n",
            "Epoch: [1][900/2539]\tBatch Time 1.000 (1.321)\tData Load Time 0.006 (0.007)\tCE Loss 13.2869 (13.1297)\tVB Loss 1.8230 (1.8101)\tF1 0.763 (0.732)\n",
            "Epoch: [1][1000/2539]\tBatch Time 1.645 (1.317)\tData Load Time 0.007 (0.007)\tCE Loss 12.9779 (13.1259)\tVB Loss 1.3282 (1.8022)\tF1 0.832 (0.735)\n",
            "Epoch: [1][1100/2539]\tBatch Time 1.467 (1.316)\tData Load Time 0.006 (0.007)\tCE Loss 12.9933 (13.1227)\tVB Loss 1.9626 (1.8001)\tF1 0.783 (0.737)\n",
            "Epoch: [1][1200/2539]\tBatch Time 1.473 (1.314)\tData Load Time 0.006 (0.007)\tCE Loss 13.1666 (13.1180)\tVB Loss 1.7798 (1.8003)\tF1 0.861 (0.738)\n",
            "Epoch: [1][1300/2539]\tBatch Time 1.136 (1.315)\tData Load Time 0.007 (0.007)\tCE Loss 13.0133 (13.1138)\tVB Loss 2.5275 (1.7927)\tF1 0.600 (0.739)\n",
            "Epoch: [1][1400/2539]\tBatch Time 1.238 (1.316)\tData Load Time 0.007 (0.007)\tCE Loss 13.2194 (13.1113)\tVB Loss 1.3816 (1.7870)\tF1 0.653 (0.740)\n",
            "Epoch: [1][1500/2539]\tBatch Time 1.390 (1.313)\tData Load Time 0.006 (0.007)\tCE Loss 13.2095 (13.1066)\tVB Loss 1.9472 (1.7851)\tF1 0.816 (0.740)\n",
            "Epoch: [1][1600/2539]\tBatch Time 1.425 (1.315)\tData Load Time 0.007 (0.007)\tCE Loss 12.9581 (13.1032)\tVB Loss 1.1498 (1.7819)\tF1 0.918 (0.740)\n",
            "Epoch: [1][1700/2539]\tBatch Time 1.552 (1.316)\tData Load Time 0.007 (0.007)\tCE Loss 12.7917 (13.1000)\tVB Loss 1.0122 (1.7799)\tF1 0.892 (0.741)\n",
            "Epoch: [1][1800/2539]\tBatch Time 1.402 (1.316)\tData Load Time 0.007 (0.007)\tCE Loss 12.9073 (13.0966)\tVB Loss 1.7496 (1.7679)\tF1 0.807 (0.742)\n",
            "Epoch: [1][1900/2539]\tBatch Time 1.293 (1.316)\tData Load Time 0.007 (0.007)\tCE Loss 13.3304 (13.0937)\tVB Loss 1.9356 (1.7628)\tF1 0.646 (0.744)\n",
            "Epoch: [1][2000/2539]\tBatch Time 1.354 (1.316)\tData Load Time 0.006 (0.007)\tCE Loss 12.8735 (13.0908)\tVB Loss 1.6698 (1.7566)\tF1 0.864 (0.745)\n",
            "Epoch: [1][2100/2539]\tBatch Time 1.359 (1.316)\tData Load Time 0.007 (0.007)\tCE Loss 12.9390 (13.0884)\tVB Loss 1.8680 (1.7517)\tF1 0.647 (0.746)\n",
            "Epoch: [1][2200/2539]\tBatch Time 1.444 (1.316)\tData Load Time 0.006 (0.007)\tCE Loss 12.8839 (13.0852)\tVB Loss 1.8107 (1.7433)\tF1 0.824 (0.747)\n",
            "Epoch: [1][2300/2539]\tBatch Time 1.563 (1.315)\tData Load Time 0.007 (0.007)\tCE Loss 13.1402 (13.0820)\tVB Loss 1.2606 (1.7359)\tF1 0.794 (0.747)\n",
            "Epoch: [1][2400/2539]\tBatch Time 1.223 (1.314)\tData Load Time 0.007 (0.007)\tCE Loss 13.2934 (13.0792)\tVB Loss 2.0095 (1.7316)\tF1 0.804 (0.747)\n",
            "Epoch: [1][2500/2539]\tBatch Time 1.815 (1.312)\tData Load Time 0.008 (0.007)\tCE Loss 12.9645 (13.0761)\tVB Loss 1.4773 (1.7276)\tF1 0.809 (0.747)\n",
            "Validation: [0/317]\tBatch Time 0.618 (0.618)\tVB Loss 0.8558 (0.8558)\tF1 Score 0.893 (0.893)\t\n",
            "Validation: [100/317]\tBatch Time 0.538 (0.537)\tVB Loss 0.9125 (1.2078)\tF1 Score 0.883 (0.826)\t\n",
            "Validation: [200/317]\tBatch Time 0.438 (0.540)\tVB Loss 1.2462 (1.1555)\tF1 Score 0.879 (0.830)\t\n",
            "Validation: [300/317]\tBatch Time 0.610 (0.543)\tVB Loss 1.0648 (1.1802)\tF1 Score 0.899 (0.830)\t\n",
            "\n",
            " * LOSS - 1.174, F1 SCORE - 0.832\n",
            "\n",
            "\n",
            "DECAYING learning rate.\n",
            "The new learning rate is 0.013636\n",
            "\n",
            "Epoch: [2][0/2539]\tBatch Time 1.285 (1.285)\tData Load Time 0.137 (0.137)\tCE Loss 13.0995 (13.0995)\tVB Loss 1.7019 (1.7019)\tF1 0.717 (0.717)\n",
            "Epoch: [2][100/2539]\tBatch Time 1.566 (1.289)\tData Load Time 0.006 (0.008)\tCE Loss 13.0256 (13.0215)\tVB Loss 2.2729 (1.5914)\tF1 0.602 (0.759)\n",
            "Epoch: [2][200/2539]\tBatch Time 1.001 (1.282)\tData Load Time 0.006 (0.007)\tCE Loss 13.2721 (13.0143)\tVB Loss 1.7828 (1.6099)\tF1 0.649 (0.761)\n",
            "Epoch: [2][300/2539]\tBatch Time 1.221 (1.291)\tData Load Time 0.007 (0.007)\tCE Loss 12.9679 (13.0017)\tVB Loss 0.6895 (1.5821)\tF1 0.676 (0.764)\n",
            "Epoch: [2][400/2539]\tBatch Time 1.228 (1.288)\tData Load Time 0.007 (0.007)\tCE Loss 12.8954 (12.9924)\tVB Loss 1.7496 (1.5599)\tF1 0.782 (0.768)\n",
            "Epoch: [2][500/2539]\tBatch Time 0.984 (1.289)\tData Load Time 0.007 (0.007)\tCE Loss 12.8961 (12.9887)\tVB Loss 0.6308 (1.5579)\tF1 0.908 (0.772)\n",
            "Epoch: [2][600/2539]\tBatch Time 1.533 (1.299)\tData Load Time 0.007 (0.007)\tCE Loss 13.1282 (12.9843)\tVB Loss 2.1108 (1.5643)\tF1 0.804 (0.771)\n",
            "Epoch: [2][700/2539]\tBatch Time 1.160 (1.303)\tData Load Time 0.006 (0.007)\tCE Loss 12.8432 (12.9813)\tVB Loss 1.6815 (1.5582)\tF1 0.793 (0.770)\n",
            "Epoch: [2][800/2539]\tBatch Time 1.552 (1.304)\tData Load Time 0.006 (0.007)\tCE Loss 13.0660 (12.9773)\tVB Loss 0.9853 (1.5526)\tF1 0.888 (0.770)\n",
            "Epoch: [2][900/2539]\tBatch Time 1.152 (1.306)\tData Load Time 0.007 (0.007)\tCE Loss 12.7177 (12.9730)\tVB Loss 1.2976 (1.5486)\tF1 0.931 (0.771)\n",
            "Epoch: [2][1000/2539]\tBatch Time 0.988 (1.303)\tData Load Time 0.007 (0.007)\tCE Loss 13.0611 (12.9701)\tVB Loss 0.8280 (1.5408)\tF1 0.940 (0.772)\n",
            "Epoch: [2][1100/2539]\tBatch Time 1.157 (1.301)\tData Load Time 0.007 (0.007)\tCE Loss 12.8734 (12.9676)\tVB Loss 1.0112 (1.5361)\tF1 0.897 (0.772)\n",
            "Epoch: [2][1200/2539]\tBatch Time 1.422 (1.299)\tData Load Time 0.009 (0.007)\tCE Loss 12.7572 (12.9652)\tVB Loss 1.0711 (1.5368)\tF1 0.869 (0.773)\n",
            "Epoch: [2][1300/2539]\tBatch Time 1.341 (1.301)\tData Load Time 0.006 (0.007)\tCE Loss 12.8425 (12.9629)\tVB Loss 0.8673 (1.5399)\tF1 0.820 (0.774)\n",
            "Epoch: [2][1400/2539]\tBatch Time 1.057 (1.299)\tData Load Time 0.006 (0.007)\tCE Loss 12.9657 (12.9595)\tVB Loss 1.3531 (1.5286)\tF1 0.791 (0.775)\n",
            "Epoch: [2][1500/2539]\tBatch Time 1.186 (1.294)\tData Load Time 0.006 (0.007)\tCE Loss 12.8128 (12.9558)\tVB Loss 1.2404 (1.5190)\tF1 0.815 (0.776)\n",
            "Epoch: [2][1600/2539]\tBatch Time 1.152 (1.292)\tData Load Time 0.006 (0.007)\tCE Loss 13.1811 (12.9516)\tVB Loss 1.3893 (1.5168)\tF1 0.577 (0.777)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process Process-8:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
            "    r = index_queue.get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7c94907cd385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-7c94907cd385>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m               \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m               vb_decoder=vb_decoder)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# One epoch's validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-7c94907cd385>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;31m# Back prop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}