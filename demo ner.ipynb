{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "def show_result(input_sentence,output_tag):\n",
    "   \n",
    "    assert(len(input_sentence) == len(output_tag))\n",
    "    for i in range(len(input_sentence)):\n",
    "        w,t = input_sentence[i],output_tag[i]\n",
    "        if t[0]==\"B\":\n",
    "            print(\"[#{} {} \".format(t[2:],w),end=\"\")\n",
    "            if (i+1>=len(output_tag) or output_tag[i+1][0]!=\"I\"):\n",
    "                print(\"]\",end=\" \")\n",
    "        elif t[0]==\"I\" and (i+1>=len(output_tag) or output_tag[i+1][0]!=\"I\"):\n",
    "            print(w,\"]\",end=\" \")\n",
    "        else:\n",
    "            print(w,end=\" \")\n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data parameters\n",
    "task = 'ner'  \n",
    "train_file = 'datasets/train.txt'  \n",
    "val_file = 'datasets/dev.txt'  \n",
    "test_file = 'datasets/test.txt' \n",
    "\n",
    "min_word_freq = 5 \n",
    "min_char_freq = 1  \n",
    "caseless = False  \n",
    "expand_vocab = True  \n",
    "\n",
    "# Model parameters\n",
    "char_emb_dim = 30 \n",
    "word_emb_dim = 100\n",
    "word_rnn_dim = 300  \n",
    "char_rnn_dim = 300  \n",
    "char_rnn_layers = 1  \n",
    "word_rnn_layers = 1 \n",
    "highway_layers = 1  \n",
    "dropout = 0.5  \n",
    "fine_tune_word_embeddings = True  \n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0  \n",
    "batch_size = 10 \n",
    "lr = 0.015  \n",
    "lr_decay = 0.05  #\n",
    "momentum = 0.9  \n",
    "workers = 1  \n",
    "epochs = 200 \n",
    "grad_clip = 5.  \n",
    "print_freq = 100  \n",
    "best_f1 = 0.  \n",
    "checkpoint =  'BESTner.pth.tar'  \n",
    "\n",
    "epochs_since_improvement = 0\n",
    "tag_ind = 3   # choose column in  dataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wsg_checkpoint = \"BESTwsg.pth.tar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsg_state = torch.load(wsg_checkpoint, map_location=lambda storage, loc: storage)\n",
    "wsg_model = wsg_state['model']\n",
    "wsg_word_map = wsg_state['word_map']\n",
    "wsg_lm_vocab_size = wsg_state['lm_vocab_size']\n",
    "wsg_tag_map = wsg_state['tag_map']\n",
    "wsg_char_map = wsg_state['char_map']\n",
    "\n",
    "def wsg(words):\n",
    "\n",
    "    global wsg_word_map, wsg_char_map, wsg_tag_map\n",
    "\n",
    "   \n",
    "    \n",
    "    #print(tag_map)\n",
    "    #print(char_map)\n",
    "    \n",
    "    \n",
    "    # Loss functions\n",
    "    lm_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    crf_criterion = ViterbiLoss(tag_map).to(device)\n",
    "\n",
    "   \n",
    "    vb_decoder = ViterbiDecoder(wsg_tag_map)\n",
    "    \n",
    "    temp_word_map = {k: v for k, v in wsg_word_map.items() if v <= wsg_word_map['<unk>']}\n",
    "    \n",
    "    \n",
    "    tags = [[\"O\"]*len(words[i]) for i in range(len(words))]\n",
    "    \n",
    "    val_inputs = create_input_tensors(words, tags, temp_word_map, wsg_char_map, wsg_tag_map)\n",
    "    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=workers, pin_memory=False)\n",
    "                   \n",
    "\n",
    "    wsg_model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    vb_losses = AverageMeter()\n",
    "    \n",
    "    start = time.time()\n",
    "    output = []\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
    "            val_loader):\n",
    "\n",
    "        max_word_len = max(wmap_lengths.tolist())\n",
    "        max_char_len = max(cmap_lengths.tolist())\n",
    "\n",
    "        wmaps = wmaps[:, :max_word_len].to(device)\n",
    "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
    "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
    "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
    "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
    "        tmaps = tmaps[:, :max_word_len].to(device)\n",
    "        wmap_lengths = wmap_lengths.to(device)\n",
    "        cmap_lengths = cmap_lengths.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = wsg_model(cmaps_f,\n",
    "                                                                                   cmaps_b,\n",
    "                                                                                   cmarkers_f,\n",
    "                                                                                   cmarkers_b,\n",
    "                                                                                   wmaps,\n",
    "                                                                                   tmaps,\n",
    "                                                                                   wmap_lengths,\n",
    "                                                                                   cmap_lengths)\n",
    "\n",
    "       \n",
    "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
    "        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
    "       \n",
    "        inv={v:k for k,v in wsg_tag_map.items()}\n",
    "        w = []\n",
    "        for j,t in enumerate([inv[k] for k in decoded.numpy()]):\n",
    "            if t==\"O\" or t==\"B\":\n",
    "                w.append(words[i][j])\n",
    "            else:\n",
    "                if len(w)==0:\n",
    "                    w.append(words[i][j])\n",
    "                else:\n",
    "                    w[-1]+=\"_\"+words[i][j]\n",
    "        output.append(w)\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "model = state['model']\n",
    "optimizer = state['optimizer']\n",
    "word_map = state['word_map']\n",
    "lm_vocab_size = state['lm_vocab_size']\n",
    "tag_map = state['tag_map']\n",
    "char_map = state['char_map']\n",
    "start_epoch = state['epoch'] + 1\n",
    "best_f1 = state['f1']\n",
    "\n",
    "val_words, val_tags = read_words_tags(val_file, 3, caseless)\n",
    "\n",
    "\n",
    "def ner(words):\n",
    "\n",
    "    global best_f1, epochs_since_improvement, state, start_epoch, word_map, char_map, tag_map\n",
    "\n",
    "   \n",
    "    \n",
    "    #print(tag_map)\n",
    "    #print(char_map)\n",
    "    \n",
    "    \n",
    "    # Loss functions\n",
    "    lm_criterion = nn.CrossEntropyLoss().to(device)\n",
    "    crf_criterion = ViterbiLoss(tag_map).to(device)\n",
    "\n",
    "   \n",
    "    vb_decoder = ViterbiDecoder(tag_map)\n",
    "    \n",
    "    temp_word_map = {k: v for k, v in word_map.items() if v <= word_map['<unk>']}\n",
    "    \n",
    "    \n",
    "    #print(\"len: \",len(val_words))\n",
    "\n",
    "    \n",
    "    tags = [[\"O\"]*len(words[0])]\n",
    "   \n",
    "    \n",
    "    val_inputs = create_input_tensors(words, tags, temp_word_map, char_map, tag_map)\n",
    "    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n",
    "                                             num_workers=workers, pin_memory=False)\n",
    "                   \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    vb_losses = AverageMeter()\n",
    "    f1s = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n",
    "            val_loader):\n",
    "\n",
    "        max_word_len = max(wmap_lengths.tolist())\n",
    "        max_char_len = max(cmap_lengths.tolist())\n",
    "\n",
    "        wmaps = wmaps[:, :max_word_len].to(device)\n",
    "        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n",
    "        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n",
    "        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n",
    "        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n",
    "        tmaps = tmaps[:, :max_word_len].to(device)\n",
    "        wmap_lengths = wmap_lengths.to(device)\n",
    "        cmap_lengths = cmap_lengths.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n",
    "                                                                                   cmaps_b,\n",
    "                                                                                   cmarkers_f,\n",
    "                                                                                   cmarkers_b,\n",
    "                                                                                   wmaps,\n",
    "                                                                                   tmaps,\n",
    "                                                                                   wmap_lengths,\n",
    "                                                                                   cmap_lengths)\n",
    "\n",
    "       \n",
    "        decoded = vb_decoder.decode(crf_scores.to(\"cpu\"), wmap_lengths_sorted.to(\"cpu\"))\n",
    "        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n",
    "       \n",
    "        inv={v:k for k,v in tag_map.items()}\n",
    "        #print([inv[k] for k in decoded.numpy()])\n",
    "        \n",
    "        print(\"INPUT:\")\n",
    "        print()\n",
    "        print(\" \".join(words[0]))\n",
    "        print()\n",
    "        \n",
    "        output = [inv[k] for k in decoded.numpy()]\n",
    "        show_result(words[0],output)\n",
    "        \n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chạy một câu ngẫu nhiên từ tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "\n",
      "Khi đến được nơi người kia nằm toán lính Mỹ nhận thấy người đó đang bảo_vệ các bệnh_nhân trong một bệnh_viện .\n",
      "\n",
      "Khi đến được nơi người kia nằm toán lính [#LOC Mỹ ] nhận thấy người đó đang bảo_vệ các bệnh_nhân trong một bệnh_viện . \n",
      "\n",
      "Khi đến được nơi người kia nằm toán lính [#LOC Mỹ ] nhận thấy người đó đang bảo_vệ các bệnh_nhân trong một bệnh_viện . \n"
     ]
    }
   ],
   "source": [
    "index = random.randrange(0,len(val_words)-1)\n",
    "words = [val_words[index]]\n",
    "ner(words)\n",
    "true_tags = [val_tags[index]]\n",
    "show_result(words[0],true_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chạy một câu từ dữ liệu nhập vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GS', 'Nguyễn', 'Minh', 'Thuyết', '-', 'Tổng', 'chủ', 'biên', 'Chương', 'trình', 'Giáo', 'dục', 'phổ', 'thông', 'cũng', 'bày', 'tỏ', 'với', 'báo', 'chí', ':', '“', 'Việc', 'công', 'nhận', 'tiếng', 'Anh', 'là', 'ngôn', 'ngữ', 'thứ', 'hai', 'của', 'Việt', 'Nam', 'trước', 'sau', 'cũng', 'phải', 'thực', 'hiện', '.', 'Công', 'nhận', 'tiếng', 'Anh', 'như', 'ngôn', 'ngữ', 'thứ', 'hai', 'sẽ', 'thúc', 'đẩy', 'việc', 'lớp', 'trẻ', 'học', 'tiếng', 'Anh', '.', 'Nếu', 'không', 'phải', 'toàn', 'dân', 'học', 'tiếng', 'Anh', 'thì', 'ít', 'nhất', 'là', 'lớp', 'trẻ', '”']\n",
      "\n",
      "[['GS', 'Nguyễn', 'Minh', 'Thuyết', '-', 'Tổng', 'chủ_biên', 'Chương_trình', 'Giáo_dục', 'phổ_thông', 'cũng', 'bày_tỏ', 'với', 'báo_chí', ':', '“', 'Việc', 'công_nhận', 'tiếng', 'Anh', 'là', 'ngôn_ngữ', 'thứ', 'hai', 'của', 'Việt_Nam', 'trước', 'sau', 'cũng', 'phải', 'thực_hiện', '.', 'Công_nhận', 'tiếng', 'Anh', 'như', 'ngôn_ngữ', 'thứ', 'hai', 'sẽ', 'thúc_đẩy', 'việc', 'lớp', 'trẻ', 'học', 'tiếng', 'Anh', '.', 'Nếu', 'không', 'phải', 'toàn', 'dân', 'học', 'tiếng', 'Anh', 'thì', 'ít_nhất', 'là', 'lớp', 'trẻ', '”']]\n",
      "\n",
      "INPUT:\n",
      "\n",
      "GS Nguyễn Minh Thuyết - Tổng chủ_biên Chương_trình Giáo_dục phổ_thông cũng bày_tỏ với báo_chí : “ Việc công_nhận tiếng Anh là ngôn_ngữ thứ hai của Việt_Nam trước sau cũng phải thực_hiện . Công_nhận tiếng Anh như ngôn_ngữ thứ hai sẽ thúc_đẩy việc lớp trẻ học tiếng Anh . Nếu không phải toàn dân học tiếng Anh thì ít_nhất là lớp trẻ ”\n",
      "\n",
      "GS [#PER Nguyễn Minh Thuyết ] - [#PER Tổng ] chủ_biên Chương_trình Giáo_dục phổ_thông cũng bày_tỏ với báo_chí : “ Việc công_nhận [#MISC tiếng Anh ] là ngôn_ngữ thứ hai của [#LOC Việt_Nam ] trước sau cũng phải thực_hiện . Công_nhận [#MISC tiếng Anh ] như ngôn_ngữ thứ hai sẽ thúc_đẩy việc lớp trẻ học [#MISC tiếng Anh ] . Nếu không phải toàn dân học [#MISC tiếng Anh ] thì ít_nhất là lớp trẻ ” \n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent = \"\"\"\n",
    "GS Nguyễn Minh Thuyết - Tổng chủ biên Chương trình Giáo dục phổ thông cũng bày tỏ với báo chí: “Việc công nhận tiếng Anh là ngôn ngữ thứ hai của Việt Nam trước sau cũng phải thực hiện. Công nhận tiếng Anh như ngôn ngữ thứ hai sẽ thúc đẩy việc lớp trẻ học tiếng Anh. Nếu không phải toàn dân học tiếng Anh thì ít nhất là lớp trẻ”\n",
    "\"\"\"\n",
    "token = nltk.word_tokenize(sent)\n",
    "print(token)\n",
    "word_segment = wsg([token])\n",
    "print()\n",
    "print(word_segment)\n",
    "print()\n",
    "ner(word_segment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
